{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the Dependencies\n",
    "### Music 21- A Toolkit for Computer-Aided Musical Analysis.\n",
    "### glob- In Python, the glob module is used to retrieve files/pathnames matching a specified pattern. The pattern rules of glob follow standard Unix path expansion rules.\n",
    "### pickle- “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy.\n",
    "### numpy - NumPy is a Python library that provides a simple yet powerful data structure\n",
    "### keras - It was developed to make implementing deep learning models as fast and easy as possible for research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from music21 import converter, instrument, note, chord, stream\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing all Files\n",
    "### The file is first converted into a stream.Score Object \n",
    "### Inside every single music file, every element is checked, if it's a Note or a Chord. The Note or the Chord are stored in an array called as Notes. \n",
    "### If the element is a chord, It is split into corresponding notes & concats them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing Classic_Guitar\\ss-01.mid\n",
      "parsing Classic_Guitar\\ss-02.mid\n",
      "parsing Classic_Guitar\\ss-03.mid\n",
      "parsing Classic_Guitar\\ss-04.mid\n",
      "parsing Classic_Guitar\\ss-05.mid\n",
      "parsing Classic_Guitar\\ss-06.mid\n",
      "parsing Classic_Guitar\\ss-07.mid\n",
      "parsing Classic_Guitar\\ss-08.mid\n",
      "parsing Classic_Guitar\\ss-09.mid\n",
      "parsing Classic_Guitar\\ss-10.mid\n",
      "parsing Classic_Guitar\\ss-11.mid\n",
      "parsing Classic_Guitar\\ss-12.mid\n",
      "parsing Classic_Guitar\\ss-13.mid\n",
      "parsing Classic_Guitar\\ss-14.mid\n",
      "parsing Classic_Guitar\\ss-15.mid\n",
      "parsing Classic_Guitar\\ss-16.mid\n",
      "parsing Classic_Guitar\\ss-17.mid\n",
      "parsing Classic_Guitar\\ss-18.mid\n",
      "parsing Classic_Guitar\\ss-19.mid\n",
      "parsing Classic_Guitar\\ss-20.mid\n"
     ]
    }
   ],
   "source": [
    "notes = []\n",
    "\n",
    "for file in glob.glob(\"Classic_Guitar/*.mid\"):\n",
    "    midi = converter.parse(file) # Convert file into stream.Score Object\n",
    "    \n",
    "    print(\"parsing %s\"%file)\n",
    "    \n",
    "    elements_to_parse = midi.flat.notes\n",
    "    \n",
    "    \n",
    "    for ele in elements_to_parse:\n",
    "        # If the element is a Note,  then store it's pitch\n",
    "        if isinstance(ele, note.Note):\n",
    "            notes.append(str(ele.pitch))\n",
    "\n",
    "        # If the element is a Chord, split each note of chord and join them with +\n",
    "        elif isinstance(ele, chord.Chord):\n",
    "            notes.append(\"+\".join(str(n) for n in ele.normalOrder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Number of notes in all the files in the MIDI Music Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9782"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Open the Filepath to later on save the Notes in a File. \n",
    "### This Process ensures that the MIDI Files do not need to be parsed again & again to find their notes, while improving the model further in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"notes_classical_guitar\", 'wb') as filepath:\n",
    "    pickle.dump(notes, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the Notes are stored in the file \"notes_classical_guitar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"notes_classical_guitar\", 'rb') as f:\n",
    "    notes= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_vocab stores the length of the unique notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(set(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Total & Unique notes are printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notes-  9782\n",
      "Unique notes-  181\n"
     ]
    }
   ],
   "source": [
    "print(\"Total notes- \", len(notes))\n",
    "print(\"Unique notes- \",  n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Range of Notes are Printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F3', '7+11+2', '0+4', '2+5', 'E3', '4+7', 'F3', 'G3', '9+0+4', '2+5', '2+5+9', '0+4', 'G3', '2+5', '7+11+2', '0', 'E4', '0+2+6', 'G4', '7+11', 'D4', '10+0+4', 'F4', '5+9', 'C4', '2+5+9', 'E4', '4+8', 'B3', '7+9+1', 'D4', '2+6', 'A3', '5+7+11', '9+0+4', '2+5+9', '5+7+11', '0+4', 'E3', '0+6', 'A4', 'B3', 'G3', '2+7', '4+10', 'G4', 'F3', 'A3', '0+5', '9+2', 'F4', '4+8', '11+4', '1+7', 'E4', 'D3', 'F#3', '9+2', '5+11', 'D4', '0+4', 'C4', '0+2+6', 'D3', 'G2', 'E3', 'C4', 'B3', 'F3', 'A3', 'B3', 'F3', '0', 'F3', 'E3', 'D3', 'E3', 'E4', 'C3', 'G3', 'E3', 'G3', 'E3', 'C4', 'G3', 'E3', 'G3', 'G4', 'C3', 'G3', 'E3', 'G3', 'E3', 'E4', 'G3', 'E3', 'C4', 'G3', 'D4', 'C3']\n"
     ]
    }
   ],
   "source": [
    "print(notes[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Sequential Data for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to build a Supervised Learning Model\n",
    "### Sequence Length- How many elements LSTM input should consider\n",
    "### Basically, LSTM Model would take in 100 Inputs & predict one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PitchNames stores all the Unique Notes (& Chords) in a sorted order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchnames = sorted(set(notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostly ML Models do work with strings\n",
    "### So, We convert the data into Integers. \n",
    "### The Following is mapping between Element (Note/Chord) to Number (Integer)\n",
    "### The Mapping is stored in a Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_to_int = dict( (ele, num) for num, ele in enumerate(pitchnames) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define Network_Input & Network_Output Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input = []\n",
    "network_output = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following, seq_in is a set of 100 Elements (Notes/Chords) & Seq_oug is the 101th Element. This continues till all the Notes have been either a part of seq_in or seq_out\n",
    "### Now, The Notes array contains strings, however, we have to work with Numerical Data, so we convert the entire data into Numericals in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(notes) - sequence_length):\n",
    "    seq_in = notes[i : i+sequence_length] # contains 100 values\n",
    "    seq_out = notes[i + sequence_length]\n",
    "    \n",
    "    network_input.append([ele_to_int[ch] for ch in seq_in])\n",
    "    network_output.append(ele_to_int[seq_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are assigning n_patterns as the length of Network_Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9682\n"
     ]
    }
   ],
   "source": [
    "# No. of examples\n",
    "n_patterns = len(network_input)\n",
    "print(n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Needs Input in 3 Dimension. \n",
    "### Now, We have Network_input & Network_Output \n",
    "### We need another Dimention, So we just take 1 as another dimention. \n",
    "### This gives us the desired shape for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9682, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Desired shape for LSTM\n",
    "network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "print(network_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Normalize the Data in the Next Step\n",
    "### Normalized Data is needed for Deep Learning Models, such as this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_network_input = network_input/float(n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network output are the classes, encode into one hot vector\n",
    "### Network Output are seperated, It is turned into one vector, using Keras np_utils.to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_output = np_utils.to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dimensions of Network_output can be seen in the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9682, 181)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dimensions of Network_output & Normalized_network_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9682, 100, 1)\n",
      "(9682, 181)\n"
     ]
    }
   ],
   "source": [
    "print(normalised_network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model\n",
    "### Importing Necessary Libraries \n",
    "### In the previous section, Data has been made from our music, this section deals with feeding the converted data into the Music\n",
    "### The Sequential model, which is very straightforward (a simple list of layers), but is limited to single-input, single-output stacks of layers (as the name gives away).\n",
    "### Layers are the basic building blocks of neural networks in Keras. A layer consists of a tensor-in tensor-out computation function (the layer's call method) and some state, held in TensorFlow variables (the layer's weights).\n",
    "### A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have declared the model to be a sequential model. Since, It is the first later, we have to define the shape of the Input. We use the Shape of Normalized_network_input \n",
    "### Sequence has to be returned, as this is not the last layer & further layers exist.\n",
    "### Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "### Dropout is easily implemented by randomly selecting nodes to be dropped-out with a given probability (e.g. 20%) each weight update cycle. This is how Dropout is implemented in Keras. Dropout is only used during the training of a model and is not used when evaluating the skill of the model.\n",
    "### Application of dropout at each layer of the network has shown good results.\n",
    "### Another LSTM Layer is added with 512 Units & Return Sequence is true, as this is another layer which is not the output layer. \n",
    "### In the last Layer, Again the Units 512 & Return sequence is false by default. \n",
    "### At the end, there's a dense layer of 256 Layers & a dropout of 0.3. \n",
    "### In the final layer, The Dense layer should have units equal to n_vocab. it should have it's activation function as softmax.\n",
    "### Softmax is typically used as the activation function when 2 or more class labels are present in the class membership in the classification of multi-class problems. It is also a general case of the sigmoid function when it represents the binary variable probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( LSTM(units=512,\n",
    "               input_shape = (normalised_network_input.shape[1], normalised_network_input.shape[2]),\n",
    "               return_sequences = True) )\n",
    "model.add( Dropout(0.3) )\n",
    "model.add( LSTM(512, return_sequences=True) )\n",
    "model.add( Dropout(0.3) )\n",
    "model.add( LSTM(512) )\n",
    "model.add( Dense(256) )\n",
    "model.add( Dropout(0.3) )\n",
    "model.add( Dense(n_vocab, activation=\"softmax\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, The Model is compiled , using the Adam Optimizer \n",
    "### The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n",
    "### The Model tries to minimize the Categorical crossentropy loss value.\n",
    "### Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Summary of the Model is shown in the following code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 512)          1052672   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 181)               46517     \n",
      "=================================================================\n",
      "Total params: 5,428,917\n",
      "Trainable params: 5,428,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code block stores the Model in the file \"model_classical_guitar.hdf5\", while the loss is monitored. The Mode is Minimum, as the loss is to be minimized. \n",
    "### The statement below the same (model.fit) is used to start the actual training. \n",
    "### The Various parameters like epochs define the number of interations of the training & the other parameters which will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x0000014C080C7070>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpu5n9gzu1.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Dense.call of <keras.layers.core.Dense object at 0x0000014C080C7070>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmpu5n9gzu1.py, line 48)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  4/152 [..............................] - ETA: 14:37 - loss: 5.1073"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5c16abcf2a8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel_his\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalised_network_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    898\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rishabh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"model_classical_guitar.hdf5\", monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "model_his = model.fit(normalised_network_input, network_output, epochs=10, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, We load the model to start making new music. \n",
    "### The Training has been done through Google Colab, to ensure that the Model Trains quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_classical_guitar.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network_Input is now an np array, It is thus declared again\n",
    "### Network_Input here is basically a list of list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7a72122da8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnetwork_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mseq_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnotes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# contains 100 values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnetwork_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mele_to_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'notes' is not defined"
     ]
    }
   ],
   "source": [
    "sequence_length = 100\n",
    "network_input = []\n",
    "\n",
    "for i in range(len(notes) - sequence_length):\n",
    "    seq_in = notes[i : i+sequence_length] # contains 100 values\n",
    "    network_input.append([ele_to_int[ch] for ch in seq_in])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Start is initialized with a random number, to generate different music every time \n",
    "### Integer to Element Mapping is created, as the Element to Integer Map was declared above. A dictionary is used again\n",
    "### Initial Pattern the starting index from where, the 100 Notes(or Chords) are taken\n",
    "### 200 Elements are generated, which should produce music for around 1 min. \n",
    "### In the for loop, initially 100 elements are taken from the start point, then prediction is made using LSTM, Now, the 1st Element is removed & the 1-101 elements are used to carry out the same process. \n",
    "### Prediction input is used to get the data into the standard format, suitable for the model. \n",
    "### Prediction gives the probability of each unique element. The Element( Note/Chord) which has the Max Probability is taken. \n",
    "### The Result is then appended to the output & the pattern is changed to include the next 100 elements, not considering the first element. The recent value predicted by the Model is Appended next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any random start index\n",
    "start = np.random.randint(len(network_input) - 1)\n",
    "\n",
    "# Mapping int_to_ele\n",
    "int_to_ele = dict((num, ele) for num, ele in enumerate(pitchnames))\n",
    "\n",
    "# Initial pattern \n",
    "pattern = network_input[start]\n",
    "prediction_output = []\n",
    "\n",
    "# generate 200 elements\n",
    "for note_index in range(300):\n",
    "    prediction_input = np.reshape(pattern, (1, len(pattern), 1)) # convert into numpy desired shape \n",
    "    prediction_input = prediction_input/float(n_vocab) # normalise\n",
    "    \n",
    "    prediction =  model.predict(prediction_input, verbose=0)\n",
    "    \n",
    "    idx = np.argmax(prediction)\n",
    "    result = int_to_ele[idx]\n",
    "    prediction_output.append(result) \n",
    "    \n",
    "    # Remove the first value, and append the recent value.. \n",
    "    # This way input is moving forward step-by-step with time..\n",
    "    pattern.append(idx)\n",
    "    pattern = pattern[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Output Elements (Notes/Chords) can be seen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'A3', 'E4', 'E4', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'E4', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3', 'A3']\n"
     ]
    }
   ],
   "source": [
    "print(prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Midi File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code section basically iterates over the Output Elements & Checks whether they are Notes or chords. \n",
    "### Output_notes is used to store the the output Notes or Chords\n",
    "## If the Element is a note, \n",
    "### A note object is made, using the class note in the note subpackage. \n",
    "### If Note is found, It is given an Offset & Instrument is set from a list of Instruments in the Music 21 Library\n",
    "### Output is appended with this New Note along with it's offset. \n",
    "## Else, if it's Chord \n",
    "### If + is a part of the patter, or the patter is a complete digit, then the pattern is a chord. \n",
    "### Every single Note is seperated out by checking where the '+' sign is. Thus, getting all the notes of the chord. \n",
    "### The array temp_notes is used to store the data of every single note & then further the list of notes (temp_notes) are converted into the corresponding chord object as new_chord\n",
    "### The Note is then appended into the output_notes array\n",
    "### Offset says, that after the first note, the second note should start playing after 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "offset = 0 # Time\n",
    "output_notes = []\n",
    "\n",
    "for pattern in prediction_output:\n",
    "    \n",
    "    # if the pattern is a chord\n",
    "    if ('+' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('+')\n",
    "        temp_notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))  # create Note object for each note in the chord\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            temp_notes.append(new_note)\n",
    "            \n",
    "        \n",
    "        new_chord = chord.Chord(temp_notes) # creates the chord() from the list of notes\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    \n",
    "    else:\n",
    "            # if the pattern is a note\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "        \n",
    "    offset = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Stream Object from the generated Notes \n",
    "## Writing the Stream Object into a test_output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_output.mid'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a stream object from the generated notes\n",
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp = \"test_output.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv67897'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv67897');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "midi_stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
